The purpose of this project was to create a pipeline that automatically reduces the data contained in the ULTRACAM archive and to make it easily accessible to interested users around the world. To that extent we have succeeded. We wrote an automated pipeline that was able to process all of the data archive and produce web pages allowing the user to browse all of the ULTRACAM runs from first-light in 2004 to the present. 

The automated pipeline enabled the checking of thousands of objects in hundreds of runs by rapidly scanning each light-curve by eye. With this process we found a few dozen objects that exhibited variability that had not yet been documented. Some of these objects are discussed in chapter \ref{chap:highlights}. We were also able to reproduce light-curves for the objects that were the original targets of the run without having prior knowledge about which were the intended targets. For example, we 'discovered' GU Mus, an X-ray binary, serendipitously, but after checking the finding charts, realised that this was a known variable and the intended target of the observer. Despite causing premature excitement, this 'error' goes some way to demonstrating that the automated pipeline is a legitimate and useful method of processing the ULTRACAM data. It enables the rapid inspection, browsing and sharing of the output of ULTRACAM. 

The pipeline lacks robust photometric reduction processes. So, while it is useful for producing light-curves that are easily inspected by eye, it does not produce calibrated magnitudes and colours. The output of the pipeline can be used as input for a final calibration task, but this needs manual intervention to find the relevant runs that contain the standard stars and to find the appropriate runs containing flat fields and biases to apply to the reduction. 

\section{Current status of the pipeline}
Running the reduction of the automated pipeline is incredibly simple. As all of the tasks are automated, one single command is all that is needed to reduce a full night's worth of data and prepare all of the web pages used for browsing that night. 

A macro script was created to allow the pipeline to make use of the internal distributed computing facility at the University of Warwick, called, Cluster of Workstations (CoWs). This has enabled us to go back through the entire ULTRACAM archive and process the vast majority of the runs. We have processed 373 nights out of a total of 406. 

\subsection{Areas for immediate improvement}
Finding a good astrometric solution for any particular field fails in many cases and, at the moment, we only have astrometric solutions for xx\% of the runs. The issues with finding good astrometric solutions are discussed in section \ref{sect:astrometry}. This is an area that could be improved by experimenting with different astrometry packages, such as Scamp from Astromatic.net \footnote{\url{http://www.astromatic.net/software/scamp}}. It is also worth trying to use different reference catalogs for Astrometry.net software if we can find bluer and deeper catalogs to index. 

ULTRACAM was designed to be used as a high speed photometric camera. Many of the science runs use very short exposure times and for runs lasting longer than 30 minutes or so, the raw data could contain more than 100,000 frames. The automated pipeline is not optimised to run across these high cadence runs. It could be argued that this is not particularly important since these runs have very few objects on them, many only have two objects in the windowed portion of the CCD, the target and a comparison. In these situations, the traditional pipeline is more suited to the task of reduction, since setting up the apertures is a very quick and easy process. Nevertheless, if we want the automated pipeline to be suited for \emph{all} of the ULTRACAM data, then some optimisation for these high cadence runs is desired. 

For runs with high cadence and for runs with many objects, the volume of data stored in the JSON files is fairly large $\sim 300$ Mbyte. This taxes the memory management of the browser. With a standard desktop machine (8 Gbyte RAM, 1.6GHz Intel i5 with 6Mbyte cache) this is not a critical problem as all of the run pages load and function in Firefox and Chrome. Nevertheless, the lag times when loading the data and plotting the light-curves can be several tens of seconds and this makes the user interface sluggish. Javascript relies on internal garbage collection to handle most of its memory management and it is clear that, in these high load runs, memory leaks will eventually cause the browser to reach a memory maximum. Although it doesn't crash, the lag becomes longer than a minute and the page needs a reload. The obvious solution to this problem is the reduce the amount of data that is loaded in to the browser. This would require a change in the application architecture to balance the load between the client (browser) and the web server. This approach would increase the amount of code required on the server and is discussed in section \ref{sect:clientserver}.  

ULTRACAM is used to observe transits of exoplanets with high precision photometry and timing. The exoplanet targets are relatively bright compared to most objects observed by ULTRACAM and, in order to avoid saturating the CCDs, the telescope is deliberately de-focussed in order to sread the light over many more pixels. The source extraction software, SExtractor) 
  
\section{Recommendations for ULTRACAM users}

Things to remember for ULTRACAM users. 
\begin{itemize}
	\item Accurate entry of the target info.
	\item Don't rotate the camera unless you have to.
	\item Large fields of view if you can. Don't worry about data size. 
\end{itemize} 

\section{Next steps}
\begin{itemize}
	\item Photometric calibration using, say, Sloan fields, citing PTF pipeline as an example.
	\item Server-side and client-side components for the browser interface to remedy memory problems.
	\item Automatic variability detection and light-curve classification. 
	\item Investigate source-extraction alternatives to cope with crowded fields and out-of-focus runs.
	\item Automatic tweaking of the source extractor parameters: The pipeline has a fairly 'brute-force' approach to reducing the photometry as it has to deal with a very diverse set of input data and therefore relies on 'best-guess' values for parameters such as aperture-size, background variability, object-detection thresholds and distance matching. Many of the reductions could be improved by tweaking these parameters to match the specific conditions of the particular run. It is conceivable that these tweaks could be automated in certain cases
	\item Ability to combine 'consecutive' runs. 
\end{itemize} 
 

