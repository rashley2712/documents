The purpose of this project was to create a pipeline that automatically reduces the data contained in the ULTRACAM archive and to makes it easily accessible to interested users around the world. To that extent it has been a success. The new  automated pipeline was able to process all of the data archive and produce web pages allowing a user to browse nearly all of the ULTRACAM runs from first-light in 2004 to the present day. 

The automated pipeline enabled checking of thousands of objects in hundreds of runs by rapidly scanning each light-curve by eye. With this tool we have already found a dozen or so objects that exhibited variability that had not yet been documented. Some of these objects are discussed in chapter \ref{chap:highlights}. We were also able to reproduce light-curves for the objects that were the original targets of the run without having prior knowledge about the purpose of these runs. For example, we `discovered' {GU Mus}, an X-ray binary, serendipitously, but after checking the finding charts, realised that this was a known variable and the intended target of the observer. Despite causing premature excitement, this error demonstrates that the automated pipeline is a legitimate and useful method of processing the ULTRACAM data. It enables rapid inspection, browsing and sharing of the output of ULTRACAM. 

The pipeline lacks robust photometric reduction processes. So, while it is useful for producing light-curves that are easily inspected by eye, it does not produce calibrated magnitudes and colours. The output of the pipeline can be used as input for a final calibration task, but this needs manual intervention. Manual steps are needed to find the relevant runs that contain the standard stars and to find the appropriate runs containing flat fields and biases to apply to the reduction. 

\section{Current status of the pipeline}
Running the reduction of the automated pipeline is simple. As all of the tasks are automated, one single command is all that is needed to reduce a full night's worth of data and prepare all of the web pages used for browsing that night. 

A macro script was created to allow the pipeline to make use of the internal distributed computing facility at the University of Warwick called Cluster of Workstations (CoWs). This has enabled us to go back through the entire ULTRACAM archive and process the vast majority of the runs. We have processed 373 nights out of a total of 406. 

\section{Areas for immediate improvement}
Finding a good astrometric solution for any particular field fails in many cases and, at the moment, there are only astrometric solutions for about 10\% of the runs. The issues with finding good astrometric solutions are discussed in section \ref{sect:astrometry}. This is an area that could be improved by experimenting with different astrometry packages, such as Scamp from Astromatic.net \footnote{\url{http://www.astromatic.net/software/scamp}}. It is also worth trying to use different reference catalogs with the Astrometry.net software if find bluer and deeper catalogs can be found. 

ULTRACAM was designed to be used as a high speed camera and some of the science runs use very short exposure times and last longer than 30 minutes or more. For these runs,  the raw data could contain more than 100,000 frames. The automated pipeline is not optimised to process these high cadence runs. It could be argued that this is not particularly important since these runs have very few target objects contained in their fields. Actually, many only have two objects in the windowed portion of the CCD, the target and a comparison. In these situations, the traditional pipeline is more suited to the task of reduction, since setting up the apertures is a very quick and easy process. Nevertheless, if the automated pipeline is to be suited for \emph{all} of the ULTRACAM data, then some optimisation for these high cadence runs is desired. 

For runs with high cadence and for runs with many objects, the volume of data stored in the JSON files is fairly large $\sim 300$ Mbyte. This taxes the memory management of the browser. With a standard desktop machine (8 Gbyte RAM, 1.6GHz Intel i5 with 6Mbyte cache) this is not a critical problem as all of the run pages load and operate correctly in Firefox and Chrome. Nevertheless, the lag times when loading the data and plotting the light-curves can be several tens of seconds and this makes the user interface sluggish. JavaScript relies on internal garbage collection to handle its memory management and it has become clear that in these high load runs, memory leaks will eventually cause the browser to reach a memory maximum. Although it doesn't crash, the lag becomes longer than a minute and the page needs to be reloaded to make it useable. The obvious solution to this problem is the reduce the amount of data that is loaded in to the browser. This would require a change in the application architecture to balance the load between the client (browser) and the web server. This approach would increase the amount of code required on the server and is discussed in section \ref{sect:clientserver}.  

In an early effort to limit the amount of data being loaded into the browser, the error estimates on the photometry were dropped from the final output. This means that the browser view of the light-curve does not include any error data and we cannot plot error bars on the light-curves. This is an urgent issue that will be addressed in the next version of the pipeline. 

ULTRACAM is used to observe transits of exoplanets with high precision photometry and timing. The exoplanet targets are relatively bright compared to most objects observed by ULTRACAM and, in order to avoid saturating the CCDs, the telescope is deliberately de-focussed to spread the light over many more pixels. This results in star images that resemble discs rather than the expected PSF of a Moffat profile. The source extraction software, SExtractor, is able to accurately segment these extended images and measure the total flux, but this requires a change to the configuration parameters. This step is not automated by the pipeline at the moment, but this is planned for the next version. 

The image in figure \ref{fig:tweakingthreshold} shows us that there are objects, revealed in the deep integrations of the fields that are not being picked up by the automated pipeline as they do not stand out sufficiently in the individual frames. A revised approach to our aperture definition should make use of these stacked images to identify fainter, but genuine, objects for photometry. 

\section{Future enhancements}
If this automated pipeline becomes popular with the ULTRACAM community and is a useful tool as part of the research then we can continue to upgrade the software. Below we list some of the potential areas for longer term enhancements.

{Photometric calibration}: If we are able to improve the astrometric solution finding of the pipeline then we could try to find known photometric standard stars in the fields and use their known fluxes to calibrate the photometry for the run. Another approach might be to search through the comments fields as entered by the principal observer (PO) on the same night looking for the text `standard' and use these as calibration runs. 

{Automatic variability detection and light-curve classification}: At the moment, we are detecting new variable objects by visual inspection of the light-curves, but this could be automated by using techniques such as those developed for automated surveys. Surveys such as the Catalina Survey are producing catalogs of variable stars found by automatically analysing and classifying millions of stars, \citep{CatalinaCatalog}.

{Alternative source extraction and flux measurement}: It would be prudent to investigate alternative methods for source extraction and flux measurement. New versions of Astropy include source extraction and photometry libraries and these will be trialed in future versions of the pipeline. 

{Automatic tweaking of the source extractor parameters}: The pipeline has a fairly 'brute-force' approach to reducing the photometry as it has to deal with a very diverse set of input data and therefore relies on best-guess values for parameters such as aperture-size, background variability, object-detection thresholds and distance matching. Many of the reductions could be improved by tweaking these parameters to match the specific conditions of the particular run. It is conceivable that these tweaks could be automated by a pipeline that first analyses the run and tries to classify into a category, such as `exoplanet transit, defocused' and `high-cadence, few objects'. These categories could have a pre-defined list of parameters appropriate for each.
 
{Ability to combine consecutive runs}: On some occasions, several consecutive runs are taken for the same target. There are also occasions where the same target has been observed multiple times over the course of several nights. It should be possible for the automated pipeline to combine these runs to produce a web page containing a combined set of light-curves for the full set of observations. 
 
\section{Recommendations for ULTRACAM users}
During the processing of the 12 year long ULTRACAM data archive, it has become clear that, by following a few simple guidelines, the observers can aid the automatic photometric reduction of the data by following some simple guidelines. 

{Accurate entry of the target info}: Finding an accurate WCS solution for each of the fields is improved when we have a world coordinate for the target object. Since the camera does not automatically acquire pointing data from the telescope, this information is dependent on the observer entering this information into the observing log, either as explicit coordinates or by providing an accurate and recognised object identifier that can later be referenced. 

{Flat-fields, biases and photometric standards}: The automated pipeline is not able to calibrate the photometry using standard stars since, at the moment, there is no reliable way to determine which runs contain measurements of relevant photometric standards. This process is performed manually in the traditional version of the pipeline. This is also true for flat-fields and for bias frames. If a standard practice of entering certain metadata into the observing logs was adopted, then we might be able to automate the application 
of flat-fields, biases and photometric standards to the automated pipeline. 

{Avoid using small windows}: Finding astrometric solutions for the fields is made very difficult when the images are very small and contain very few objects in them. Where possible, the observer should try to include as much of the full field as is reasonable. 

\section{Summary}
ULTRACAM has taken a large amount of observations over the last 12 years. This has created a rich dataset that is a valuable resource to be mined. With an automated reduction pipeline we can explore this archive. Publishing photometric data to the web is a convenient way of accessing, sharing and exploring scientific information and will be useful for future observations that ULTRACAM makes. Hopefully this project will be beneficial to the ULTRACAM community. 